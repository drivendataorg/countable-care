{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "requirements are listed in requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/xgboost/wrapper/')\n",
    "import xgboost as xgb\n",
    "from multiprocessing import Pool\n",
    "import sklearn\n",
    "import random\n",
    "\n",
    "def pool(func, arg, N=15):\n",
    "    mpool = Pool(N)\n",
    "    ans = mpool.map(func, arg)\n",
    "    mpool.terminate()\n",
    "    return ans\n",
    "\n",
    "def sigm(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = read_csv('data/train_values.csv',',',low_memory=False)\n",
    "test = read_csv('data/test_values.csv',',',low_memory=False)\n",
    "labels = read_csv('data/train_labels.csv',',')\n",
    "sampleSubmission = read_csv('data/SubmissionFormat.csv',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xcols = list(train.columns)\n",
    "xcols.remove('id')\n",
    "\n",
    "ycols = list(labels.columns)\n",
    "ycols.remove('id')\n",
    "\n",
    "#Drop columns that has one one unique value\n",
    "for c in train.columns:\n",
    "    if train[c].unique().shape[0] == 1:\n",
    "        xcols.remove(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform Data using DictVectorizer\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [dict(row) for i,row in train[xcols].fillna(-1).iterrows()]\n",
    "trainV = v.fit_transform(D)\n",
    "\n",
    "D = [dict(row) for i,row in test[xcols].fillna(-1).iterrows()]\n",
    "testV = v.transform(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate feature importances using Random Forest\n",
    "\n",
    "rf_impV = []\n",
    "rf = RandomForestClassifier(n_estimators=500, n_jobs=15, criterion='entropy', max_features=100)\n",
    "\n",
    "loss = []\n",
    "for i,c in enumerate(ycols):\n",
    "    rf.fit(trainV, labels[c])\n",
    "    rf_impV += [rf.feature_importances_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Meta-features & Stacking"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Stacking is a technique of combining predictions of different classifiers in new meta-classifier.\n",
    "\n",
    "http://en.wikipedia.org/wiki/Ensemble_learning#Stacking\n",
    "\n",
    "Here I am doing so in four steps:\n",
    "(1) split train dataset in two parts : train_part_1 and train_part_2\n",
    "(2) fit algorithm on train_part_1, make prediction on train_part_2 and test\n",
    "(3) fit algorithm on train_part_2, make prediction on train_part_1 and test\n",
    "(4) average two predictions for test set recieved on step (2) and (3), thus recieving prediction for all train and test\n",
    "\n",
    "After averaging predictions recieved on step (4) with different random splits in step (1) we are making new meta-feature which will be used in out meta-classifier\n",
    "\n",
    "In this section I make such features using next classifiers:\n",
    "/order of classifiers is the same as orders of cells in this section/\n",
    "(1) RandomForestClassifier(n_estimators=100, n_jobs=15, criterion='entropy', max_features=200)\n",
    "(2) RandomForestClassifier(n_estimators=100, n_jobs=15, criterion='entropy', max_features=300)\n",
    "(3) LogisticRegression(C=0.01)\n",
    "(4) LinearSVC()\n",
    "(5) PassiveAggressiveClassifier()\n",
    "(6) XGBoost classifier (see parameters in corresponding cell)\n",
    "\n",
    "Each of them is averaging N times (see in cells).\n",
    "In addition, I use feature selection based on RandomForest's feature importances ('impcols' variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast(i):\n",
    "    random.seed(i)\n",
    "\n",
    "    mask = train_test_split(np.arange(trainV.shape[0]), test_size=.5, random_state=i)\n",
    "    ansmask = []\n",
    "    ytrain_i = np.zeros(trainV.shape[0])\n",
    "    for m1,m2 in [(mask[0],mask[1]), (mask[1],mask[0])]:\n",
    "        \n",
    "        alg.fit(trainV[impcols].values[m1], labels[c].values[m1])\n",
    "    \n",
    "        ytrain_i[m2] = sigm(alg.decision_function(trainV[impcols].values[m2]))\n",
    "        ansmask += [sigm(alg.decision_function(testV[impcols].values))]\n",
    "    return ytrain_i, sum(ansmask)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(n_estimators=100, n_jobs=15, criterion='entropy', max_features=200)\n",
    "\n",
    "N = 50\n",
    "ytrain_N = np.zeros((trainV.shape[0], 14, N))\n",
    "ycv_N = np.zeros((testV.shape[0], 14, N))\n",
    "\n",
    "for nc, c in enumerate(ycols):\n",
    "    impcols = np.arange(trainV.shape[1])[rf_impV[nc] > 0.0001]    \n",
    "    \n",
    "    for i in range(N):\n",
    "        mask = train_test_split(np.arange(trainV.shape[0]), test_size=.5)\n",
    "        ansmask = []\n",
    "        for m1,m2 in [(mask[0],mask[1]), (mask[1],mask[0])]:\n",
    "            alg.fit(trainV[impcols].values[m1], labels[c].values[m1])\n",
    "            ytrain_N[m2,nc,i] = alg.predict_proba(trainV[impcols].values[m2])[:,1]\n",
    "            ansmask += [alg.predict_proba(testV[impcols].values)[:,1]]\n",
    "        ycv_N[:,nc,i] = sum(ansmask)/2\n",
    "    \n",
    "Mtrain = df(ytrain_N.mean(axis=2), columns=ycols)\n",
    "Mtest = df(ycv_N.mean(axis=2), columns=ycols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alg = RandomForestClassifier(n_estimators=100, n_jobs=15, criterion='entropy', max_features=300)\n",
    "\n",
    "N = 50\n",
    "ytrain_N = np.zeros((trainV.shape[0], 14, N))\n",
    "ycv_N = np.zeros((testV.shape[0], 14, N))\n",
    "\n",
    "for nc, c in enumerate(ycols):\n",
    "    impcols = np.arange(trainV.shape[1])[rf_impV[nc] > 0.00001]    \n",
    "    \n",
    "    for i in range(N):\n",
    "        mask = train_test_split(np.arange(trainV.shape[0]), test_size=.5)\n",
    "        ansmask = []\n",
    "        for m1,m2 in [(mask[0],mask[1]), (mask[1],mask[0])]:\n",
    "            alg.fit(trainV[impcols].values[m1], labels[c].values[m1])\n",
    "            ytrain_N[m2,nc,i] = alg.predict_proba(trainV[impcols].values[m2])[:,1]\n",
    "            ansmask += [alg.predict_proba(testV[impcols].values)[:,1]]\n",
    "        ycv_N[:,nc,i] = sum(ansmask)/2\n",
    "    \n",
    "Mtrain2 = df(ytrain_N.mean(axis=2), columns=ycols)\n",
    "Mtest2 = df(ycv_N.mean(axis=2), columns=ycols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alg = sklearn.linear_model.LogisticRegression(C=0.01)\n",
    "\n",
    "N = 100\n",
    "ytrain_N = np.zeros((trainV.shape[0], 14))\n",
    "ycv_N = np.zeros((testV.shape[0], 14))\n",
    "\n",
    "for nc, c in enumerate(ycols):\n",
    "    impcols = np.array(trainV.columns)[rf_impV[nc] > 0.0001]\n",
    "    \n",
    "    ans = pool(fast,range(N))\n",
    "    ytrain_N[:,nc] = sum([i[0] for i in ans]) / N\n",
    "    ycv_N[:,nc] = sum([i[1] for i in ans]) / N\n",
    "    \n",
    "MtrainLin = df(ytrain_N, columns=ycols)\n",
    "MtestLin = df(ycv_N, columns=ycols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alg = sklearn.svm.LinearSVC()\n",
    "\n",
    "N = 100\n",
    "ytrain_N = np.zeros((trainV.shape[0], 14))\n",
    "ycv_N = np.zeros((testV.shape[0], 14))\n",
    "\n",
    "for nc, c in enumerate(ycols):\n",
    "    impcols = np.array(trainV.columns)[rf_impV[nc] > 0.0001]\n",
    "    \n",
    "    ans = pool(fast,range(N))\n",
    "    ytrain_N[:,nc] = sum([i[0] for i in ans]) / N\n",
    "    ycv_N[:,nc] = sum([i[1] for i in ans]) / N\n",
    "    \n",
    "MtrainLin2 = df(ytrain_N, columns=ycols)\n",
    "MtestLin2 = df(ycv_N, columns=ycols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alg = sklearn.linear_model.PassiveAggressiveClassifier()\n",
    "\n",
    "N = 100\n",
    "ytrain_N = np.zeros((trainV.shape[0], 14))\n",
    "ycv_N = np.zeros((testV.shape[0], 14))\n",
    "\n",
    "for nc, c in enumerate(ycols):\n",
    "    impcols = np.array(trainV.columns)[rf_impV[nc] > 0.0001]\n",
    "    \n",
    "    ans = pool(fast,range(N))\n",
    "    ytrain_N[:,nc] = sum([i[0] for i in ans]) / N\n",
    "    ycv_N[:,nc] = sum([i[1] for i in ans]) / N\n",
    "    \n",
    "MtrainLin4 = df(ytrain_N, columns=ycols)\n",
    "MtestLin4 = df(ycv_N, columns=ycols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = {}\n",
    "param['booster'] = 'gbtree'\n",
    "param['objective'] = 'binary:logistic'\n",
    "param['eval_metric'] = 'logloss'\n",
    "param['scale_pos_weight'] = 1.0\n",
    "param['bst:eta'] = 0.25\n",
    "param['bst:max_depth'] = 5\n",
    "param['bst:colsample_bytree'] = 0.25\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 15\n",
    "\n",
    "num_round = 120\n",
    "plst = list(param.items())\n",
    "\n",
    "N = 50\n",
    "ytrain_N = np.zeros((trainV.shape[0], 14, N))\n",
    "ycv_N = np.zeros((testV.shape[0], 14, N))\n",
    "\n",
    "for nc, c in enumerate(ycols):\n",
    "    impcols = np.arange(trainV.shape[1])[rf_impV[nc] > 0.0001]\n",
    "    \n",
    "    Gcv = xgb.DMatrix( testV.values, missing = -1 )\n",
    "    \n",
    "    for i in range(N):\n",
    "        mask = train_test_split(np.arange(trainV.shape[0]), test_size=.5)\n",
    "        m1, m2 = mask[0], mask[1]\n",
    "        \n",
    "        Gtrain1 = xgb.DMatrix( trainV.values[m1], label = labels[c].values[m1], missing = -1 )\n",
    "        Gtrain2 = xgb.DMatrix( trainV.values[m2], label = labels[c].values[m2], missing = -1 )\n",
    "        \n",
    "        ansmask = []\n",
    "        for t1,t2,m in [(Gtrain1, Gtrain2, m2), (Gtrain2, Gtrain1, m1)]:\n",
    "            \n",
    "            bst = xgb.train( plst, t1, num_round )#, watchlist )\n",
    "            ytrain_N[m,nc,i] = bst.predict( t2 )\n",
    "            ansmask += [bst.predict( Gcv )]        \n",
    "            \n",
    "        ycv_N[:,nc,i] = sum(ansmask)/2\n",
    "\n",
    "MtrainX = df(ytrain_N.mean(axis=2), columns=ycols)\n",
    "MtestX = df(ycv_N.mean(axis=2), columns=ycols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. Meta-classifiers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this section I use classifiers and meta-classifiers build upon data after DictVectorizer and meta-features recieved in the previous section:\n",
    "/order of classifiers is the same as orders of cells in this section/\n",
    "\n",
    "(1) Xgboost fit on features and meta-features (xgbFM)\n",
    "(2) Neural Network fit on meta-features (nnM)\n",
    "(3) ExtraTreesClassifier fit on features and meta-features (rfFM)\n",
    "(4) ExtraTreesClassifier fit on features (rfF)\n",
    "(5) Xgboost fit on features (xgbF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = sampleSubmission.copy()\n",
    "\n",
    "param = {}\n",
    "param['booster'] = 'gbtree'\n",
    "param['objective'] = 'binary:logistic'\n",
    "param['eval_metric'] = 'logloss'\n",
    "param['scale_pos_weight'] = 1\n",
    "param['bst:eta'] = 0.2\n",
    "param['bst:max_depth'] = 2\n",
    "param['bst:colsample_bytree'] = 1\n",
    "param['bst:subsample'] = 1\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 15\n",
    "\n",
    "num_rounds = [105, 50, 50, 60, 65, 80, 40, 45, 45, 55, 35, 40, 40, 55]\n",
    "plst = list(param.items())\n",
    "\n",
    "for nc, c in enumerate(ycols):\n",
    "    Gtrain = xgb.DMatrix( np.concatenate([trainV, Mtrain, Mtrain2, MtrainLin, MtrainLin2, MtrainLin4, MtrainX], axis=1), label = labels[c].values, missing = -1 )\n",
    "    Gtest = xgb.DMatrix( np.concatenate([testV, Mtest, Mtest2, MtestLin, MtestLin2, MtestLin4, MtestX], axis=1), missing = -1 )\n",
    "\n",
    "    bst = xgb.train( plst, Gtrain, num_rounds[nc] )\n",
    "    pred = bst.predict( Gtest )\n",
    "    submission[c] = pred\n",
    "\n",
    "submission.to_csv('submissions/xgbFM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theanets\n",
    "\n",
    "loss = []\n",
    "nn_predF = []\n",
    "for nc,c in enumerate(ycols):\n",
    "\n",
    "    ntrain = np.concatenate([Mtrain2, Mtrain, MtrainLin, MtrainLin2, MtrainLin4, MtrainX], axis=1).astype(np.float32)\n",
    "    ncv = np.concatenate([Mtest2, Mtest, MtestLin, MtestLin2, MtestLin4, MtestX], axis=1).astype(np.float32)\n",
    "    exp = theanets.Experiment(\n",
    "        theanets.Classifier,\n",
    "        layers=(6*14, 5, 2),\n",
    "        train_batches = 1000,\n",
    "    )\n",
    "\n",
    "    exp.train(\n",
    "        (ntrain, labels[c].values.astype(np.int32)),\n",
    "    )\n",
    "\n",
    "    pred = exp.network.predict(ncv)[:,1]\n",
    "    nn_predF += [pred]\n",
    "    \n",
    "submission = sampleSubmission.copy()\n",
    "\n",
    "for nc,c in enumerate(ycols):\n",
    "    submission[c] = nn_predF[nc]\n",
    "    \n",
    "submission.to_csv('submissions/nnM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = sampleSubmission.copy()\n",
    "rf = ExtraTreesClassifier(n_estimators=1000, n_jobs=15, criterion='entropy', max_features=200)\n",
    "\n",
    "for nc,c in enumerate(ycols):\n",
    "    imp_cols = np.arange(trainV.shape[1])[rf_impV[nc] > 0.0001]\n",
    "    \n",
    "    rf.fit(trainV[imp_cols], labels[c])\n",
    "    pred = rf.predict_proba(testV[imp_cols])[:,1]\n",
    "         \n",
    "    submission[c] = pred\n",
    "    \n",
    "submission.to_csv('submissions/rfF.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = sampleSubmission.copy()\n",
    "\n",
    "param = {}\n",
    "param['booster'] = 'gbtree'\n",
    "param['objective'] = 'binary:logistic'\n",
    "param['eval_metric'] = 'logloss'\n",
    "param['scale_pos_weight'] = 1.0\n",
    "param['bst:eta'] = 0.25\n",
    "param['bst:max_depth'] = 5\n",
    "param['bst:colsample_bytree'] = 0.25\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 15\n",
    "\n",
    "num_round = 120\n",
    "plst = list(param.items())\n",
    "\n",
    "xgb_predF = []\n",
    "\n",
    "loss = []\n",
    "for i,c in enumerate(ycols[:]):\n",
    "\n",
    "    Gtrain = xgb.DMatrix( trainV.values, label = labels[c].values, missing = -1 )\n",
    "    Gtest = xgb.DMatrix( testV.values, missing = -1 )\n",
    "    \n",
    "    bst = xgb.train( plst, Gtrain, num_round )\n",
    "    pred = bst.predict( Gtest )\n",
    "    \n",
    "    submission[c] = pred\n",
    "    \n",
    "submission.to_csv('submissions/xgbF.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.info('')\n",
    "\n",
    "submission = sampleSubmission.copy()\n",
    "\n",
    "rf = ExtraTreesClassifier(n_estimators=1000, n_jobs=15, criterion='entropy', max_features=320)\n",
    "\n",
    "for c in ycols:\n",
    "    impcols = np.arange(trainV.shape[1])[rf_impV[nc] > 0.0001]\n",
    "    rf.fit(np.concatenate([trainV[impcols], Mtrain, Mtrain2, MtrainLin, MtrainLin2, MtrainLin4, MtrainX], axis=1), labels[c])\n",
    "    pred = rf.predict_proba(np.concatenate([testV[impcols], Mtest, Mtest2, MtestLin, MtestLin2, MtestLin4, MtestX], axis=1))[:,1]\n",
    "\n",
    "    submission[c] = pred\n",
    "    \n",
    "submission.to_csv('submissions/rfFM.csv',index=False)\n",
    "\n",
    "logging.info('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5. Averaging predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub1 = read_csv('submissions/nnM.csv')\n",
    "sub2 = read_csv('submissions/rfFM.csv')\n",
    "sub3 = read_csv('submissions/xgbFM.csv')\n",
    "sub4 = read_csv('submissions/rfF.csv')\n",
    "sub5 = read_csv('submissions/xgbF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissionMix = sampleSubmission.copy()\n",
    "\n",
    "for c in ycols:\n",
    "    pred = (sub1[c].values * 1/3 + sub2[c].values * 1/3 + sub3[c].values * 1/3) * .75 + .25 * (sub4[c] * .5 + .5 * sub5[c])\n",
    "    \n",
    "    submissionMix[c] = pred\n",
    "    \n",
    "submissionMix.to_csv('submissions/final_submit.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
